---
title: "Overview"
author: "Jeffrey B. Arnold"
date: "July 27, 2015"
output: html_document
---

$$
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
$$

# Model Terminology

These models are called either state space models (econometrics) or
dynamic (linear) models (statistics / Bayesian).

The following defines a \emph{state space model} 
$$
  \begin{aligned}[t]
    y_t = f(\theta_t \vert  b_{t}, F_t, \nu_t) \\
    \theta_t = f(\theta_{t-1} \vert g_{t},  G_t, \nu_t)
  \end{aligned}
$$

If $\theta_t$ is continuous then it is a \emph{continuous state space
model}, if $\theta_t$ is discrete then it is a \emph{discrete state
space model}.

If those equations can be written as 
$$
\begin{aligned}[t]
y_t = b_{t} + F_t \theta_t + \nu_t \\
\theta_t = g_{t} + G_t \theta_{t-1} + \omega_t
\end{aligned}
$$
then the model is a \emph{Dynamic Linear Model (DLM)} (linear SSM), otherwise it is a non-linear dynamic model.
If $\nu_t$ and $\omega_t$ are normal distributions, then it is \emph{Guassian} or \emph{Normal Dynamic Linear Model} (GDLM or NDLM). 

A dynamic linear model is defined by the following set of equations,
$$
\label{eq:2}
\begin{aligned}[t]
  y_t &= b_{t} + F_t \theta_{t-1} + \nu_t & \nu_t & \sim N(0, V_t) \\
  \theta_t &= g_{t} + G_t \theta_{t-1} + \omega_t & \omega_t & \sim N(0, W_t) \\
  \theta_0 &\sim N(m_0, C_0)
\end{aligned}
$$
where equation \ref{eq:2} is the observation or measurement equation,
equation \ref{eq:3} is the system equation, 
and equation \ref{eq:4} is the initial information.
The number of variables is $r$ and the number of states is $p$.

\begin{table}
  \centering
  \begin{tabular}[c]{@{}ll@{}}
    \hline\noalign{\medskip}
    matrix & dimensions
    \\\noalign{\medskip}
    \hline\noalign{\medskip}
    $F_t$ & $r \times p$
    \\\noalign{\medskip}
    $G_t$ & $p \times p$
    \\\noalign{\medskip}
    $V_t$ & $r \times r$
    \\\noalign{\medskip}
    $W_t$ & $p \times p$
    \\\noalign{\medskip}
    $C_0$ & $p \times p$
    \\\noalign{\medskip}
    \hline
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \begin{tabular}[c]{@{}ll@{}}
    \hline\noalign{\medskip}
    vector & dimensions
    \\\noalign{\medskip}
    \hline\noalign{\medskip}
    $Y_t$ & $r$
    \\\noalign{\medskip}
    $\theta_t$ & $p$
    \\\noalign{\medskip}
    $b_t$ & $r$
    \\\noalign{\medskip}
    $\nu_t$ & $r$
    \\\noalign{\medskip}
    $g_t$ & $p$\
    \\\noalign{\medskip}
    $\omega_t$ & $p$
    \\\noalign{\medskip}
    $m_0$ & $p$
    \\\noalign{\medskip}
    \hline
  \end{tabular}
\end{table}


# Filtering equations

See \textcite[Chapter 2.7, p. 53]{PetrisPetroneEtAl2009} and \textcite[Chapter 4]{WestHarrison1997} for proofs.

Assume the posterior distribution at $t-1$. 
Let $\theta_{t-1} | y_{1:t-1} \sim N(m_{t-1}, C_{t-1})$.
The one step ahead predictive distribution of $\theta_{t}$ given $y_{1:t-1}$ is $N(a_{t}, R_{t})$,
$$
\begin{aligned}[t]
  a_{t} &= \E(\theta_{t} | y_{1:t-1}) &&= g_{t} + G_{t} m_{t-1} \\
  R_{t} &= \Var(\theta_{t} | y_{1:t-1}) &&= G_{t} C_{t-1} G'_{t} + W_{t}
\end{aligned}
$$
The one step ahead predictive distribution of $Y_{t}$ given $y_{1:t-1}$ is $N(f_{t}, Q_{t})$,
$$
\begin{aligned}[t]
  f_{t} &= \E(Y_{t} | y_{1:t-1}) &&= b_{t} + F_{t} a_{t} \\
  Q_{t} &= \Var(Y_{t} | y_{1:t-1}) &&= F_{t} R_{t} F'_{t} + V_{t}
\end{aligned}
$$
The filtered distribution of $\theta_{t}$ given $y_{1:t}$ is $N(m_{t}, C_{t})$
$$
\begin{aligned}[t]
  \label{eq:5}
  m_{t} &= \E(\theta_{t} | y_{1:t}) &&= a_{t} + R_{t} F'_{t} Q_{t}^{-1} e_{t} \\
  &&& = a_{t} + K_{t} e_{t} \\
  \label{eq:6}
  C_{t} &= \Var(\theta_{t} | y_{1:t}) &&= R_{t} - R_{t} F'_{t} Q_{t}^{-1} F_{t} R_{t} \\
  &&& = R_{t} - K_{t} Q_{t} K'_{t} \\
  \label{eq:1}  
  &&& = (I_{p} - K_{t} F_{t}) R_{t} (I_{p} - K_{t} F_{t})' + K_{t} V_{t} K_{t}' \\
  e_{t} &&&= y_{t} - f_{t} \\
  K_{t} &&&= R_{t} F_{t}' Q_{t}^{-1}
\end{aligned}
$$
where $e_{t}$ is the one step forecast error and $K_{t}$ is the Kalman gain (adaptative coefficient).
Equation \ref{eq:1} is the ``Joseph stablized form'' \parencite[3]{Tusell2011}.

\begin{table}
  \centering
  \begin{tabular}[]{ll}
    \hline
    variable & dim \\
    \hline
    $a_{t}$ & $p$ \tabularnewline
    $R_{t}$ & $p, p$ \tabularnewline
    $f_{t}$ & $r$ \tabularnewline
    $Q_{t}$ & $r, r$ \tabularnewline
    $m_{t}$ & $p$ \tabularnewline
    $C_{t}$ & $p, p$ \tabularnewline
    $e_{t}$ & $r$ \tabularnewline
    $K_{t}$ & $p, r$ 
  \end{tabular}
  \label{Dimensions of variables in the filtering equations.}
\end{table}

A few key identies \parencite[106-107]{WestHarrison1997} are,
$$
\begin{aligned}[t]
  K_{t} &= R_{t} F'_{t} Q_{t}^{-1} = C_{t} F'_{t} V_{t}^{-1} \\
  C_{t} &= R_{t}^{-1} - K_{t} Q_{t} K_{t} = R_{t}(I_{p} - F_{t} K_{t}') \\
  C_{t}^{-1} &= R_{t}^{-1} + F_{t}' V_{t}^{-1} F_{t} \\
  Q_{t} &= (I_{r} - F_{t} K_{t})^{-1} V_{t} \\
  F_{t} K_{t} &= I_{r} - V_{t} Q_{t}^{-1}
\end{aligned}
$$
<!-- Replace $a_{t} = G_{t} m_{t - 1} + \E(\omega_{t})$ and --> 

\subsection{Missing Values}

Missing values are equivalent to setting $F_{t} = 0$ or $V_{t} = \infty$.

If all values in $t$ are missing, replace the filter steps (equations \ref{eq:5} and \ref{eq:6}) with,
$$
\begin{aligned}[t]
  m_{t} = a_{t} \\
  C_{t} = R_{t} 
\end{aligned}
$$

Suppose some, but not all variables in time $t$ are missing.
Let $r_{t} \in [0, r]$ is the number of non-missing values in each time period, $r_{t} = \sum_{j = 1}^{r} (y_{j,t} \neq \emptyset)$.
If some are missing (let $r > r_{t} > 0$ be observed), let $M$ be a $r_{t} \times r$ selection matrix and define,
$$
\begin{aligned}[t]
  y^{*}_{t} &= M_{t} y_{t} \\
  F^{*}_{t} &= M_{t} F_{t} \\
  V^{*}_{t} &= M_{t} V_{t} M_{t}'
\end{aligned}
$$
and replace $y_{t}$, $F_{t}$ and $V_{t}$ in the filtering equations with those.
Note that the smoothing (section \ref{sec:smoothing}) and backward sampling (section \ref{sec:backward-sampling}) algorithms do not depend on $F$, $V$ and $y$ and thus do not need to be altered when there are missing values.


\subsection{Likelihood}
\label{sec:likelihood}

If there are no missing values, then the log likelihood is,
$
  L(y_{1:n}) =  - \frac{n r}{2} \log (2 \pi) - \frac{1}{2} \sum_{t=1}^{n}
  \left(
    \log | Q_{t} | + e_{t}' Q_{t}^{-1} e_{t}
  \right)
$
If there are missing values,
$
  L(y_{1:n}) = 
  -\frac{1}{2} \sum_{t=1}^{n} 
  (r_{t} > 0) 
  \left(
    r_{t} \log (2 \pi)
    + \log | Q_{t} | + e_{t}' Q_{t}^{-1} e_{t}
  \right)
$

See \textcite[Chapter 5, p. 57]{KoopmanShephardDoornik2008}.


# Smoothing

If $\theta_{t+1} | y_{1:n} \sim N(s_{t+1}, S_{t+1})$, then $\theta_{t} | y_{1:n} \sim N(s_{t}, S_{t})$ where
$$
\begin{aligned}[t]
  s_{t} &= \E(\theta_{t} | y_{1:n}) &&= m_{t} + C_{t} G'_{t+1} R_{t+1}^{-1}(s_{t+1} - a_{t+1}) \\
  S_{t} &= \Var(\theta_{t} | y_{1:n}) &&= C_{t} - C_{t} G'_{t+1} R^{-1}_{t+1} (R_{t+1} - S_{t+1}) R^{-1}_{t+1} G_{t+1} C_{t}
\end{aligned}
$$
Note that $s_{n} = m_{n}$ and $S_{n} = C_{n}$.
See \textcite[Prop 2.4, p. 61]{PetrisPetroneEtAl2009} for a proof.


# Backward Sampling

Supposing that $m_{1:n}$, $C_{1:n}$, $a_{1:n}$ and $R_{1:n}$ have been calculated by the filter,%
\footnote{No additional adjustment for intercepts is required because they are already incorporated in $a_{t}$}
To draw $\theta_{1:n} | y_{1:n}$,


#. From the Kalman filter, $\theta_{n} | y_{1:n} \sim N(m_{n}, C_{n})$
#. For $t = (n-1), \dots, 0$, $\theta_{t} | y_{1:n} \sim N(h_{t}, H_{t})$ where
    $$
    \begin{aligned}[t]
      h_{t} &= m_{t} + C_{t} G'_{t + 1} R_{t+1}^{-1}(\theta_{t+1} - a_{t+1}) \\
      H_{t} &= C_{t} - C_{t} G'_{t + 1} R_{t+1}^{-1} G_{t+1} C_{t}
    \end{aligned}
    $$

See \textcite[Chapter 4.4.1, p. 161]{PetrisPetroneEtAl2009} for more details.

# Sequential Estimation

First, consider the case in which all $V_{t}$ are diagonal.
The vector series $y_{1}, \dots, y_{n}$ is treated as a scalar series
$
  y_{1,1}, \dots, y_{1,r}, y_{2,1}, \dots, y_{n,r}
$

The prior distribution of the state is $\theta_{t} | y_{1:(t-1)} \sim N(a_{t}, R_{t})$,
$$
\begin{aligned}[t]
  a_{t} &= \E(\theta_{t} | y_{1:t-1}) &&= g_{t} + G_{t} m_{t-1} \\
  R_{t} &= \Var(\theta_{t} | y_{1:t-1}) &&= G_{t} C_{t-1} G'_{t} + W_{t}
\end{aligned}
$$

For each variable, $i = 1, \dots, r$.
The prediction equation is $y_{t,i} | y_{1:t-1}, y_{t,j|j < i} \sim N(f_{t,i}, Q_{t,i})$.
Note that $f_{t,i}$ and $Q_{t,i}$ are scalars.
$$
\begin{aligned}[t]
  f_{t,i} &= \E(Y_{t,i} | y_{1:t-1}, y_{t,j | j < i}) &&= b_{t,i} + F_{t,i} m_{t,i-1} \\
  Q_{t,i} &= \Var(Y_{t,i} | y_{1:t-1}, y_{t, j | j < i}) &&= F_{t,i} C_{t,i-1} F'_{t,i} + v_{t,i}
\end{aligned}
$$
For the first variable, let $m_{t,0} = a_{t}$ and $C_{t,0} = R_{t}$.
The filtered distribution of the latent state is $\theta_{t} | y_{1:t-1}, y_{t,j|j < i} \sim N(m_{t,i}, C_{t,i})$,
$$
\begin{aligned}[t]
  m_{t,i} &= \E(\theta_{t} | y_{1:t-1}, y_{t, j | j \leq i}) &&= m_{t,i-1} + C_{t,i-1} F'_{t,i} Q_{t,i}^{-1} e_{t,i} \\
  &&& = m_{t,i-1} + K_{t,i} e_{t,i} \\
  C_{t,i} &= \Var(\theta_{t} | y_{1:t-1}, y_{t, j|j \leq i}) &&= C_{t,i-1} - C_{t,i-1} F'_{t,i} Q_{t,i}^{-1} F_{t,i} C_{t,i-1} \\
  &&& = Q_{t,i} - K_{t,i} Q_{t,i} K'_{t,i} \\  
  &&& = (I_{n} - K_{t,i} F_{t,i}) C_{t,i} (I_{n} - K_{t,i} F_{t,i})' + K_{t,i} v_{t,i} K_{t,i}' \\
  e_{t,i} &&&= y_{t,i} - f_{t,i} \\
  K_{t,i} &&&= C_{t,i-i} F_{t,i}' Q_{t,i}^{-1}
\end{aligned}
$$
Define $m_{t} = m_{t,r}$ and $C_{t} = C_{t,r}$.
If $y_{t,i}$ is missing, then 
$$
\begin{aligned}[t]
  m_{t,i} &= m_{t,i-1} \\
  C_{t,i} &= C_{t,i-1}
\end{aligned}
$$

The likelihood in the sequential case is
$$
  L(y_{1:n}) = -\frac{1}{2} \sum_{t=1}^{n} \sum_{i=1}^{r} (y_{i,j} \neq \emptyset)
  \left( 
    \log (2 \pi) + \log |Q_{t,i}| + \frac{e_{t,i}^{2}}{Q_{t,i}} 
  \right)
$$

If $V_{t}$ is not diagonal, then diagonalize it with the Cholesky decomposition of $V_{t}$, such that $V_{t} = L_{t} D_{t} L'_{t}$ where $D_{t}$ is diagonal, and $L_{t}$ is lower triangular.
$$
\begin{aligned}[t]
  y_{t}^{*} &= L_{t}^{-1} y_{t} \\
  F_{t}^{*} &= L_{t}^{-1} F_{t} \\
  b_{t}^{*} &= L_{t}^{-1} b_{t} \\
  \epsilon^{*} & = L_{t}^{-1} \epsilon_{t}  \sim N(0, D_{t})
\end{aligned}
$$



## Univariate Local Level Model

For the univariate local level model, with $r = n = 1$, $F_{t} = G_{t} = 1$,
and $g_{t} = b_{t} = 0$, the calculations can be simplified.

One step ahead predictive distribution of $\theta_{t-1}$ given $y_{1:t-1}$ is $N(a_{t}, R_{t})$
$$
\begin{aligned}[t]
  a_{t} &= E(\theta_{t} | y_{1:t-1}) = m_{t-1} \\
  R_{t} &= Var(\theta_{t} | y_{1:t-1}) = C_{t-1} + W_{t}
\end{aligned}
$$

The One step ahead predictive distribution of $Y_{t}$ given $y_{1:t-1}$ is $N(f_{t}, Q_{t})$
$$
\begin{aligned}[t]
  f_{t} &= E(Y_{t} | y_{1:t-1}) = a_{t} = m_{t-1} \\
  Q_{t} &= Var(Y_{t} | y_{1:t-1}) = R_{t} + V_{t} = C_{t-1} + W_{t} + V_{t}
\end{aligned}
$$

The posterior distribution of $\theta_{t}$ given $y_{1:t}$ is $N(m_{t}, C_{t})$
$$
\begin{aligned}[t]
  f_{t} &= E(\theta_{t} | y_{1:t}) = a_{t} + R_{t} Q_{t}^{-1} e_{t} = m_{t-1} + \frac{C_{t-1} + W_{t}}{C_{t-1} + W_{t} + V_{t}} e_{t} \\
  Q_{t} &= Var(\theta_{t} | y_{1:t}) = R_{t} - \frac{(C_{t-1} + W_{t})^{2}}{C_{t-1} + W_{t} + V_{t}}
\end{aligned}
$$
where $e_{t} = Y_{t} - f_{t}$. The Kalman gain is defined as $K_{t} = R_{t} / Q_{t}$.
